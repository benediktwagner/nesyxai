{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7576,
     "status": "ok",
     "timestamp": 1595319959933,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "GrGrfnZIKLOl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import logging;logging.basicConfig(level=logging.INFO)\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14070,
     "status": "ok",
     "timestamp": 1595319969043,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "WxBWsJJWSgTe",
    "outputId": "10dfa4b2-3240-4de7-c383-07215161d980"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    }
   ],
   "source": [
    "## import dataset\n",
    "dataset_used = \"adult\" # \"adult\", \"german\", \"compas\"\n",
    "protected_attribute_used = 1 # 1, 2\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    dataset_orig = AdultDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "    \n",
    "elif dataset_used == \"german\":\n",
    "    dataset_orig = GermanDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "    \n",
    "elif dataset_used == \"compas\":\n",
    "    dataset_orig = CompasDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14380,
     "status": "ok",
     "timestamp": 1595319972782,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "2RFdt_YYdcdO"
   },
   "outputs": [],
   "source": [
    "import logictensornetworks2 as ltn\n",
    "import logictensornetworks2.fuzzy_ops as fuzzy_ops\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12371,
     "status": "ok",
     "timestamp": 1595319972782,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "8aR71jqd5UKX"
   },
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_ConnectiveOp(fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Implies_Reichenbach())\n",
    "Equiv = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Equiv(And,Implies))\n",
    "Forall = ltn.experimental.Wrapper_AggregationOp(ltn.experimental.Aggreg_pMeanError(p=5))\n",
    "Exists = ltn.experimental.Wrapper_AggregationOp(ltn.experimental.Aggreg_pMean(p=5))\n",
    "scale_orig = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(input):\n",
    "  var_data = ltn.variable(\"input\", input)\n",
    "  result = D(var_data)\n",
    "  return result.numpy()\n",
    "\n",
    "class Predpred(object):\n",
    "  def __init__(self, oracle):\n",
    "        self.oracle = oracle\n",
    "  def predict(self, data):\n",
    "      var_data = ltn.variable(\"input\", data)\n",
    "      result = self.oracle(var_data)\n",
    "      y_test_pred_prob = result.numpy()\n",
    "      class_thresh = 0.5\n",
    "      y_test_pred = np.zeros_like(y_test_pred_prob)\n",
    "      y_test_pred[y_test_pred_prob >= class_thresh] = 1\n",
    "      y_test_pred[~(y_test_pred_prob >= class_thresh)] = 0\n",
    "      return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "Xs = scale_orig.fit_transform(dataset_orig.features)\n",
    "ys = dataset_orig.labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.482 Epoch 0: Train-Accuracy 0.735\n",
      "Epoch 200: Sat Level 0.571 Epoch 200: Train-Accuracy 0.836\n",
      "Epoch 400: Sat Level 0.587 Epoch 400: Train-Accuracy 0.851\n",
      "Epoch 600: Sat Level 0.598 Epoch 600: Train-Accuracy 0.887\n",
      "Epoch 800: Sat Level 0.574 Epoch 800: Train-Accuracy 0.855\n",
      "Epoch 1000: Sat Level 0.601 Epoch 1000: Train-Accuracy 0.893\n",
      "Epoch 1200: Sat Level 0.603 Epoch 1200: Train-Accuracy 0.901\n",
      "Epoch 1400: Sat Level 0.609 Epoch 1400: Train-Accuracy 0.898\n",
      "Epoch 1600: Sat Level 0.610 Epoch 1600: Train-Accuracy 0.918\n",
      "Epoch 1800: Sat Level 0.610 Epoch 1800: Train-Accuracy 0.889\n",
      "Epoch 2000: Sat Level 0.613 Epoch 2000: Train-Accuracy 0.898\n",
      "Epoch 2200: Sat Level 0.617 Epoch 2200: Train-Accuracy 0.918\n",
      "Epoch 2400: Sat Level 0.613 Epoch 2400: Train-Accuracy 0.901\n",
      "Epoch 2600: Sat Level 0.619 Epoch 2600: Train-Accuracy 0.918\n",
      "Epoch 2800: Sat Level 0.619 Epoch 2800: Train-Accuracy 0.929\n",
      "Epoch 3000: Sat Level 0.616 Epoch 3000: Train-Accuracy 0.918\n",
      "Epoch 3200: Sat Level 0.618 Epoch 3200: Train-Accuracy 0.905\n",
      "Epoch 3400: Sat Level 0.617 Epoch 3400: Train-Accuracy 0.909\n",
      "Epoch 3600: Sat Level 0.620 Epoch 3600: Train-Accuracy 0.935\n",
      "Epoch 3800: Sat Level 0.619 Epoch 3800: Train-Accuracy 0.927\n",
      "Training finished at Epoch 3999 with Sat Level 0.620\n",
      "Epoch 0: Sat Level 0.500 Epoch 0: Train-Accuracy 0.871\n",
      "Epoch 200: Sat Level 0.572 Epoch 200: Train-Accuracy 0.910\n",
      "Epoch 400: Sat Level 0.579 Epoch 400: Train-Accuracy 0.913\n",
      "Epoch 600: Sat Level 0.584 Epoch 600: Train-Accuracy 0.921\n",
      "Epoch 800: Sat Level 0.586 Epoch 800: Train-Accuracy 0.917\n",
      "Epoch 1000: Sat Level 0.586 Epoch 1000: Train-Accuracy 0.923\n",
      "Epoch 1200: Sat Level 0.589 Epoch 1200: Train-Accuracy 0.924\n",
      "Epoch 1400: Sat Level 0.573 Epoch 1400: Train-Accuracy 0.916\n",
      "Epoch 1600: Sat Level 0.589 Epoch 1600: Train-Accuracy 0.934\n",
      "Epoch 1800: Sat Level 0.591 Epoch 1800: Train-Accuracy 0.933\n",
      "Epoch 2000: Sat Level 0.591 Epoch 2000: Train-Accuracy 0.936\n",
      "Epoch 2200: Sat Level 0.590 Epoch 2200: Train-Accuracy 0.924\n",
      "Epoch 2400: Sat Level 0.591 Epoch 2400: Train-Accuracy 0.937\n",
      "Epoch 2600: Sat Level 0.591 Epoch 2600: Train-Accuracy 0.919\n",
      "Epoch 2800: Sat Level 0.592 Epoch 2800: Train-Accuracy 0.937\n",
      "Epoch 3000: Sat Level 0.592 Epoch 3000: Train-Accuracy 0.938\n",
      "Epoch 3200: Sat Level 0.591 Epoch 3200: Train-Accuracy 0.908\n",
      "Epoch 3400: Sat Level 0.594 Epoch 3400: Train-Accuracy 0.920\n",
      "Epoch 3600: Sat Level 0.543 Epoch 3600: Train-Accuracy 0.906\n",
      "Epoch 3800: Sat Level 0.593 Epoch 3800: Train-Accuracy 0.940\n",
      "Training finished at Epoch 3999 with Sat Level 0.593\n",
      "Epoch 0: Sat Level 0.514 Epoch 0: Train-Accuracy 0.895\n",
      "Epoch 200: Sat Level 0.575 Epoch 200: Train-Accuracy 0.918\n",
      "Epoch 400: Sat Level 0.569 Epoch 400: Train-Accuracy 0.897\n",
      "Epoch 600: Sat Level 0.582 Epoch 600: Train-Accuracy 0.926\n",
      "Epoch 800: Sat Level 0.584 Epoch 800: Train-Accuracy 0.927\n",
      "Epoch 1000: Sat Level 0.571 Epoch 1000: Train-Accuracy 0.904\n",
      "Epoch 1200: Sat Level 0.584 Epoch 1200: Train-Accuracy 0.938\n",
      "Epoch 1400: Sat Level 0.585 Epoch 1400: Train-Accuracy 0.940\n",
      "Epoch 1600: Sat Level 0.584 Epoch 1600: Train-Accuracy 0.939\n",
      "Epoch 1800: Sat Level 0.586 Epoch 1800: Train-Accuracy 0.940\n",
      "Epoch 2000: Sat Level 0.587 Epoch 2000: Train-Accuracy 0.939\n",
      "Epoch 2200: Sat Level 0.588 Epoch 2200: Train-Accuracy 0.932\n",
      "Epoch 2400: Sat Level 0.587 Epoch 2400: Train-Accuracy 0.941\n",
      "Epoch 2600: Sat Level 0.588 Epoch 2600: Train-Accuracy 0.929\n",
      "Epoch 2800: Sat Level 0.585 Epoch 2800: Train-Accuracy 0.920\n",
      "Epoch 3000: Sat Level 0.524 Epoch 3000: Train-Accuracy 0.887\n",
      "Epoch 3200: Sat Level 0.589 Epoch 3200: Train-Accuracy 0.921\n",
      "Epoch 3400: Sat Level 0.588 Epoch 3400: Train-Accuracy 0.920\n",
      "Epoch 3600: Sat Level 0.588 Epoch 3600: Train-Accuracy 0.944\n",
      "Epoch 3800: Sat Level 0.588 Epoch 3800: Train-Accuracy 0.919\n",
      "Training finished at Epoch 3999 with Sat Level 0.589\n",
      "Epoch 0: Sat Level 0.520 Epoch 0: Train-Accuracy 0.879\n",
      "Epoch 200: Sat Level 0.576 Epoch 200: Train-Accuracy 0.909\n",
      "Epoch 400: Sat Level 0.580 Epoch 400: Train-Accuracy 0.939\n",
      "Epoch 600: Sat Level 0.584 Epoch 600: Train-Accuracy 0.939\n",
      "Epoch 800: Sat Level 0.584 Epoch 800: Train-Accuracy 0.942\n",
      "Epoch 1000: Sat Level 0.586 Epoch 1000: Train-Accuracy 0.927\n",
      "Epoch 1200: Sat Level 0.587 Epoch 1200: Train-Accuracy 0.932\n",
      "Epoch 1400: Sat Level 0.569 Epoch 1400: Train-Accuracy 0.928\n",
      "Epoch 1600: Sat Level 0.586 Epoch 1600: Train-Accuracy 0.943\n",
      "Epoch 1800: Sat Level 0.568 Epoch 1800: Train-Accuracy 0.910\n",
      "Epoch 2000: Sat Level 0.586 Epoch 2000: Train-Accuracy 0.922\n",
      "Epoch 2200: Sat Level 0.590 Epoch 2200: Train-Accuracy 0.934\n",
      "Epoch 2400: Sat Level 0.537 Epoch 2400: Train-Accuracy 0.896\n",
      "Epoch 2600: Sat Level 0.588 Epoch 2600: Train-Accuracy 0.919\n",
      "Epoch 2800: Sat Level 0.588 Epoch 2800: Train-Accuracy 0.945\n",
      "Epoch 3000: Sat Level 0.580 Epoch 3000: Train-Accuracy 0.923\n",
      "Epoch 3200: Sat Level 0.587 Epoch 3200: Train-Accuracy 0.945\n",
      "Epoch 3400: Sat Level 0.587 Epoch 3400: Train-Accuracy 0.945\n",
      "Epoch 3600: Sat Level 0.588 Epoch 3600: Train-Accuracy 0.927\n",
      "Epoch 3800: Sat Level 0.582 Epoch 3800: Train-Accuracy 0.920\n",
      "Training finished at Epoch 3999 with Sat Level 0.589\n",
      "Epoch 0: Sat Level 0.523 Epoch 0: Train-Accuracy 0.889\n",
      "Epoch 200: Sat Level 0.577 Epoch 200: Train-Accuracy 0.930\n",
      "Epoch 400: Sat Level 0.576 Epoch 400: Train-Accuracy 0.914\n",
      "Epoch 600: Sat Level 0.580 Epoch 600: Train-Accuracy 0.927\n",
      "Epoch 800: Sat Level 0.582 Epoch 800: Train-Accuracy 0.935\n",
      "Epoch 1000: Sat Level 0.572 Epoch 1000: Train-Accuracy 0.924\n",
      "Epoch 1200: Sat Level 0.581 Epoch 1200: Train-Accuracy 0.918\n",
      "Epoch 1400: Sat Level 0.579 Epoch 1400: Train-Accuracy 0.918\n",
      "Epoch 1600: Sat Level 0.584 Epoch 1600: Train-Accuracy 0.935\n",
      "Epoch 1800: Sat Level 0.582 Epoch 1800: Train-Accuracy 0.931\n",
      "Epoch 2000: Sat Level 0.583 Epoch 2000: Train-Accuracy 0.924\n",
      "Epoch 2200: Sat Level 0.573 Epoch 2200: Train-Accuracy 0.924\n",
      "Epoch 2400: Sat Level 0.581 Epoch 2400: Train-Accuracy 0.916\n",
      "Epoch 2600: Sat Level 0.583 Epoch 2600: Train-Accuracy 0.933\n",
      "Epoch 2800: Sat Level 0.584 Epoch 2800: Train-Accuracy 0.927\n",
      "Epoch 3000: Sat Level 0.572 Epoch 3000: Train-Accuracy 0.937\n",
      "Epoch 3200: Sat Level 0.582 Epoch 3200: Train-Accuracy 0.917\n",
      "Epoch 3400: Sat Level 0.552 Epoch 3400: Train-Accuracy 0.916\n",
      "Epoch 3600: Sat Level 0.586 Epoch 3600: Train-Accuracy 0.948\n",
      "Epoch 3800: Sat Level 0.524 Epoch 3800: Train-Accuracy 0.910\n",
      "Training finished at Epoch 3999 with Sat Level 0.587\n"
     ]
    }
   ],
   "source": [
    "D = ltn.Predicate.MLP([98],hidden_layer_sizes=(100,50))\n",
    "trainable_variables = D.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=5)\n",
    "\n",
    "kv = {}\n",
    "for k, (train, test) in enumerate(kf.split(Xs, ys)):\n",
    "    kv[\"posits{0}\".format(k)] = Xs[train][ys[train]==1].astype(np.float32)\n",
    "    kv[\"negats{0}\".format(k)] = Xs[train][ys[train]==0].astype(np.float32)\n",
    "    kv[\"Xtrain{0}\".format(k)] = Xs[train].astype(np.float32)\n",
    "    kv[\"ytrain{0}\".format(k)] = ys[train].astype(np.float32)\n",
    "    kv[\"Xtest{0}\".format(k)] = Xs[test].astype(np.float32)\n",
    "    kv[\"ytest{0}\".format(k)] = ys[test].astype(np.float32)\n",
    "    \n",
    "    var_posit = ltn.variable(\"posits\",kv[\"posits{0}\".format(k)])\n",
    "    var_negat = ltn.variable(\"negats\",kv[\"negats{0}\".format(k)])\n",
    "    oracle = Predpred(D)\n",
    "    \n",
    "    @tf.function\n",
    "    @ltn.domains()\n",
    "    def axioms():\n",
    "        axioms = []\n",
    "        weights = []\n",
    "        # forall data_A: A(data_A)\n",
    "        axioms.append(Forall(ltn.bound(var_posit), D(var_posit)))\n",
    "        # forall data_B: B(data_B)\n",
    "        axioms.append(Forall(ltn.bound(var_negat), Not(D(var_negat))))\n",
    "        axioms = tf.stack([tf.squeeze(ax) for ax in axioms])\n",
    "        sat_level = formula_aggregator(axioms)\n",
    "        return sat_level, axioms\n",
    "\n",
    "    for epoch in range(4000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 1. - axioms()[0]\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        if epoch%200 == 0:\n",
    "            print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()[0]),\n",
    "                  \"Epoch %d: Train-Accuracy %.3f\"%(epoch, \n",
    "                                                   accuracy_score(kv[\"ytrain{0}\".format(k)],\n",
    "                                                                  oracle.predict(kv[\"Xtrain{0}\".format(k)]))))\n",
    "    print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()[0]))\n",
    "    kv[\"test_acc{0}\".format(k)] = accuracy_score(kv[\"ytest{0}\".format(k)], oracle.predict(kv[\"Xtest{0}\".format(k)]))\n",
    "    \n",
    "    dataset_orig_test_pred = dataset_orig.subset(test).copy(deepcopy=True)\n",
    "    dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.subset(test).features))\n",
    "    classified_metric_debiasing_test = ClassificationMetric(dataset_orig.subset(test), \n",
    "                                                     dataset_orig_test_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "    \n",
    "    kv[\"Disparate_impact{0}\".format(k)] = classified_metric_debiasing_test.disparate_impact()\n",
    "    kv[\"Parity_Difference{0}\".format(k)] = classified_metric_debiasing_test.statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average 5-fold accuracy as in Padala and Gujar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8282038918386592"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"test_acc0\"]+kv[\"test_acc1\"]+kv[\"test_acc2\"]+kv[\"test_acc3\"]+kv[\"test_acc4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average 5-fold Disparity Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32559911367025146"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Disparate_impact0\"]+kv[\"Disparate_impact1\"]+kv[\"Disparate_impact2\"]+kv[\"Disparate_impact3\"]+kv[\"Disparate_impact4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average 5-fold Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26363315735698584"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Parity_Difference0\"]+kv[\"Parity_Difference1\"]+kv[\"Parity_Difference2\"]+kv[\"Parity_Difference3\"]+kv[\"Parity_Difference4\"])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model - with bias - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Classification accuracy = 0.907059\n",
      "Dataset: Disparate impact = 0.309725\n",
      "Test set: Parity Difference = -0.282242\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_test_pred = dataset_orig.copy(deepcopy=True)\n",
    "dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.features))\n",
    "display(Markdown(\"#### Model - with bias - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig, \n",
    "                                                 dataset_orig_test_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Dataset: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "print(\"Dataset: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Parity Difference = %f\" % classified_metric_debiasing_test.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.DataFrame(Xs,columns=dataset_orig.feature_names)\n",
    "X_r_preds = f(np.asarray(Xs).astype(np.float32))\n",
    "X_df['customer_risk_pred'] = X_r_preds\n",
    "X_female_df = X_df[X_df['sex'] == X_df['sex'].unique()[0]]\n",
    "X_male_df = X_df[X_df['sex'] == X_df['sex'].unique()[1]]\n",
    "X_female_df['customer class'] = pd.qcut(X_female_df['customer_risk_pred'],5, labels=[0,1,2,3,4])\n",
    "X_male_df['customer class'] = pd.qcut(X_male_df['customer_risk_pred'],5, labels=[0,1,2,3,4])\n",
    "X_inp = pd.concat([X_male_df,X_female_df])\n",
    "class1F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 0]\n",
    "class2F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 1]\n",
    "class3F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 2]\n",
    "class4F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 3]\n",
    "class5F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 4]\n",
    "class1M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 0]\n",
    "class2M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 1]\n",
    "class3M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 2]\n",
    "class4M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 3]\n",
    "class5M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpclass1f = class1F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass2f = class2F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass3f = class3F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass4f = class4F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass5f = class5F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "\n",
    "inpclass1m = class1M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass2m = class2M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass3m = class3M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass4m = class4M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass5m = class5M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "\n",
    "var_class1f = ltn.variable(\"?class1F\",inpclass1f)\n",
    "var_class2f = ltn.variable(\"?class2F\",inpclass2f)\n",
    "var_class3f = ltn.variable(\"?class3F\",inpclass3f)\n",
    "var_class4f = ltn.variable(\"?class4F\",inpclass4f)\n",
    "var_class5f = ltn.variable(\"?class5F\",inpclass5f)\n",
    "\n",
    "var_class1m = ltn.variable(\"?class1M\",inpclass1m)\n",
    "var_class2m = ltn.variable(\"?class2M\",inpclass2m)\n",
    "var_class3m = ltn.variable(\"?class3M\",inpclass3m)\n",
    "var_class4m = ltn.variable(\"?class4M\",inpclass4m)\n",
    "var_class5m = ltn.variable(\"?class5M\",inpclass5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ltn.Predicate.MLP([58],hidden_layer_sizes=(50,25))\n",
    "trainable_variables = D.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.600 Epoch 0: Train-Accuracy 0.822\n",
      "Epoch 200: Sat Level 0.712 Epoch 200: Train-Accuracy 0.886\n",
      "Epoch 400: Sat Level 0.795 Epoch 400: Train-Accuracy 0.881\n",
      "Epoch 600: Sat Level 0.803 Epoch 600: Train-Accuracy 0.877\n",
      "Epoch 800: Sat Level 0.795 Epoch 800: Train-Accuracy 0.881\n",
      "Training finished at Epoch 999 with Sat Level 0.800 Epoch 999: Test-Accuracy 0.879\n",
      "Epoch 0: Sat Level 0.788 Epoch 0: Train-Accuracy 0.881\n",
      "Epoch 200: Sat Level 0.794 Epoch 200: Train-Accuracy 0.881\n",
      "Epoch 400: Sat Level 0.787 Epoch 400: Train-Accuracy 0.874\n",
      "Epoch 600: Sat Level 0.789 Epoch 600: Train-Accuracy 0.874\n",
      "Epoch 800: Sat Level 0.791 Epoch 800: Train-Accuracy 0.874\n",
      "Training finished at Epoch 999 with Sat Level 0.806 Epoch 999: Test-Accuracy 0.887\n",
      "Epoch 0: Sat Level 0.802 Epoch 0: Train-Accuracy 0.876\n",
      "Epoch 200: Sat Level 0.802 Epoch 200: Train-Accuracy 0.875\n",
      "Epoch 400: Sat Level 0.803 Epoch 400: Train-Accuracy 0.875\n",
      "Epoch 600: Sat Level 0.806 Epoch 600: Train-Accuracy 0.876\n",
      "Epoch 800: Sat Level 0.804 Epoch 800: Train-Accuracy 0.875\n",
      "Training finished at Epoch 999 with Sat Level 0.806 Epoch 999: Test-Accuracy 0.889\n",
      "Epoch 0: Sat Level 0.806 Epoch 0: Train-Accuracy 0.878\n",
      "Epoch 200: Sat Level 0.807 Epoch 200: Train-Accuracy 0.876\n",
      "Epoch 400: Sat Level 0.811 Epoch 400: Train-Accuracy 0.876\n",
      "Epoch 600: Sat Level 0.812 Epoch 600: Train-Accuracy 0.876\n",
      "Epoch 800: Sat Level 0.812 Epoch 800: Train-Accuracy 0.876\n",
      "Training finished at Epoch 999 with Sat Level 0.808 Epoch 999: Test-Accuracy 0.889\n",
      "Epoch 0: Sat Level 0.807 Epoch 0: Train-Accuracy 0.888\n",
      "Epoch 200: Sat Level 0.810 Epoch 200: Train-Accuracy 0.887\n",
      "Epoch 400: Sat Level 0.811 Epoch 400: Train-Accuracy 0.887\n",
      "Epoch 600: Sat Level 0.812 Epoch 600: Train-Accuracy 0.887\n",
      "Epoch 800: Sat Level 0.812 Epoch 800: Train-Accuracy 0.887\n",
      "Training finished at Epoch 999 with Sat Level 0.810 Epoch 999: Test-Accuracy 0.843\n"
     ]
    }
   ],
   "source": [
    "kv = {}\n",
    "for k, (train, test) in enumerate(kf.split(Xs, ys)):\n",
    "    kv[\"posits{0}\".format(k)] = Xs[train][ys[train]==1].astype(np.float32)\n",
    "    kv[\"negats{0}\".format(k)] = Xs[train][ys[train]==0].astype(np.float32)\n",
    "    kv[\"Xtrain{0}\".format(k)] = Xs[train].astype(np.float32)\n",
    "    kv[\"ytrain{0}\".format(k)] = ys[train].astype(np.float32)\n",
    "    kv[\"Xtest{0}\".format(k)] = Xs[test].astype(np.float32)\n",
    "    kv[\"ytest{0}\".format(k)] = ys[test].astype(np.float32)\n",
    "    \n",
    "\n",
    "    var_posit = ltn.variable(\"posits\",kv[\"posits{0}\".format(k)])\n",
    "    var_negat = ltn.variable(\"negats\",kv[\"negats{0}\".format(k)])\n",
    "\n",
    "    @tf.function\n",
    "    @ltn.domains()\n",
    "    def axioms():\n",
    "        axioms = []\n",
    "        weights = []\n",
    "        # forall data_A: A(data_A)\n",
    "        axioms.append(Forall(ltn.bound(var_posit), D(var_posit)))\n",
    "        weights.append(2.)\n",
    "        # forall data_B: B(data_B)\n",
    "        axioms.append(Forall(ltn.bound(var_negat), Not(D(var_negat))))\n",
    "        weights.append(2.)\n",
    "        # Equality Constraints\n",
    "        axioms.append(Forall(ltn.bound(var_class1f,var_class1m), Equiv(D(var_class1f),D(var_class1m)),p=2))\n",
    "        weights.append(1.)\n",
    "        axioms.append(Forall(ltn.bound(var_class2f,var_class2m), Equiv(D(var_class2f),D(var_class2m)),p=2))\n",
    "        weights.append(1.)\n",
    "        axioms.append(Forall(ltn.bound(var_class3f,var_class3m), Equiv(D(var_class3f),D(var_class3m)),p=2))\n",
    "        weights.append(1.)\n",
    "        axioms.append(Forall(ltn.bound(var_class4f,var_class4m), Equiv(D(var_class4f),D(var_class4m)),p=2))\n",
    "        weights.append(1.)\n",
    "        axioms.append(Forall(ltn.bound(var_class5f,var_class5m), Equiv(D(var_class5f),D(var_class5m)),p=2))\n",
    "        weights.append(1.)\n",
    "        axioms = tf.stack([tf.squeeze(ax) for ax in axioms])\n",
    "        weights = tf.stack(weights)\n",
    "        weighted_axioms = weights*axioms\n",
    "        sat_level = formula_aggregator(weighted_axioms)\n",
    "        return sat_level, axioms\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 1. - axioms()[0]\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        if epoch%200 == 0:\n",
    "            print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()[0]),\n",
    "                  \"Epoch %d: Train-Accuracy %.3f\"%(epoch, \n",
    "                                                   accuracy_score(kv[\"ytrain{0}\".format(k)],\n",
    "                                                                  oracle.predict(kv[\"Xtrain{0}\".format(k)]))))\n",
    "    print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()[0]),\"Epoch %d: Test-Accuracy %.3f\"%(epoch, accuracy_score(kv[\"ytest{0}\".format(k)], oracle.predict(kv[\"Xtest{0}\".format(k)]))))\n",
    "    kv[\"test_acc{0}\".format(k)] = accuracy_score(kv[\"ytest{0}\".format(k)], oracle.predict(kv[\"Xtest{0}\".format(k)]))\n",
    "    \n",
    "    dataset_orig_test_pred = dataset_orig.subset(test).copy(deepcopy=True)\n",
    "    dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.subset(test).features))\n",
    "    classified_metric_debiasing_test = ClassificationMetric(dataset_orig.subset(test), \n",
    "                                                     dataset_orig_test_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "    \n",
    "    kv[\"Disparate_impact{0}\".format(k)] = classified_metric_debiasing_test.disparate_impact()\n",
    "    kv[\"Parity_Difference{0}\".format(k)] = classified_metric_debiasing_test.statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773160977754111"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"test_acc0\"]+kv[\"test_acc1\"]+kv[\"test_acc2\"]+kv[\"test_acc3\"]+kv[\"test_acc4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0131566437682546"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Disparate_impact0\"]+kv[\"Disparate_impact1\"]+kv[\"Disparate_impact2\"]+kv[\"Disparate_impact3\"]+kv[\"Disparate_impact4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Disparate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002311877056149153"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Parity_Difference0\"]+kv[\"Parity_Difference1\"]+kv[\"Parity_Difference2\"]+kv[\"Parity_Difference3\"]+kv[\"Parity_Difference4\"])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model - without bias - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Classification accuracy = 0.880368\n",
      "Dataset: Disparate impact = 0.992118\n",
      "Dataset: Parity Difference = -0.001589\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_test_pred = dataset_orig.copy(deepcopy=True)\n",
    "dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.features))\n",
    "display(Markdown(\"#### Model - without bias - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig, \n",
    "                                                 dataset_orig_test_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Dataset: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "print(\"Dataset: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Dataset: Parity Difference = %f\" % classified_metric_debiasing_test.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPvjTe7yG+k8kXL2c8+ARTd",
   "collapsed_sections": [],
   "name": "LTN2_FairnessMeasures",
   "provenance": [
    {
     "file_id": "1iixzamRwIEOAxXzhtFq98EwKZaauIEkj",
     "timestamp": 1595253671546
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
