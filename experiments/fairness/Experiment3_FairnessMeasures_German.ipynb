{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7576,
     "status": "ok",
     "timestamp": 1595319959933,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "GrGrfnZIKLOl"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import logging;logging.basicConfig(level=logging.INFO)\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14070,
     "status": "ok",
     "timestamp": 1595319969043,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "WxBWsJJWSgTe",
    "outputId": "10dfa4b2-3240-4de7-c383-07215161d980"
   },
   "outputs": [],
   "source": [
    "dataset_used = \"german\" # \"adult\", \"german\", \"compas\"\n",
    "protected_attribute_used = 1 # 1, 2\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    dataset_orig = AdultDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "    \n",
    "elif dataset_used == \"german\":\n",
    "    dataset_orig = GermanDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "    \n",
    "elif dataset_used == \"compas\":\n",
    "    dataset_orig = CompasDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14380,
     "status": "ok",
     "timestamp": 1595319972782,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "2RFdt_YYdcdO"
   },
   "outputs": [],
   "source": [
    "import logictensornetworks2 as ltn\n",
    "import logictensornetworks2.fuzzy_ops as fuzzy_ops\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12371,
     "status": "ok",
     "timestamp": 1595319972782,
     "user": {
      "displayName": "Benedikt J. Wagner",
      "photoUrl": "",
      "userId": "17708217500073810160"
     },
     "user_tz": -60
    },
    "id": "8aR71jqd5UKX"
   },
   "outputs": [],
   "source": [
    "Not = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_ConnectiveOp(fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Implies_Reichenbach())\n",
    "Equiv = ltn.Wrapper_ConnectiveOp(fuzzy_ops.Equiv(And,Implies))\n",
    "Forall = ltn.experimental.Wrapper_AggregationOp(ltn.experimental.Aggreg_pMeanError(p=5))\n",
    "Exists = ltn.experimental.Wrapper_AggregationOp(ltn.experimental.Aggreg_pMean(p=5))\n",
    "scale_orig = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(input):\n",
    "      var_data = ltn.variable(\"input\", input)\n",
    "      result = D(var_data)\n",
    "      return result.numpy()\n",
    "\n",
    "class Predpred(object):\n",
    "  def __init__(self, oracle):\n",
    "        self.oracle = oracle\n",
    "  def predict(self, data):\n",
    "      var_data = ltn.variable(\"input\", data)\n",
    "      result = self.oracle(var_data)\n",
    "      y_test_pred_prob = result.numpy()\n",
    "      class_thresh = 0.5\n",
    "      y_test_pred = np.zeros_like(y_test_pred_prob)\n",
    "      y_test_pred[y_test_pred_prob >= class_thresh] = dataset_orig.favorable_label\n",
    "      y_test_pred[~(y_test_pred_prob >= class_thresh)] = dataset_orig.unfavorable_label\n",
    "      return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "Xs = scale_orig.fit_transform(dataset_orig.features)\n",
    "ys = dataset_orig.labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.433 Epoch 0: Train-Accuracy 0.542\n",
      "Epoch 200: Sat Level 0.996 Epoch 200: Train-Accuracy 1.000\n",
      "Epoch 400: Sat Level 0.999 Epoch 400: Train-Accuracy 1.000\n",
      "Epoch 600: Sat Level 0.999 Epoch 600: Train-Accuracy 1.000\n",
      "Epoch 800: Sat Level 1.000 Epoch 800: Train-Accuracy 1.000\n",
      "Training finished at Epoch 999 with Sat Level 1.000\n",
      "Epoch 0: Sat Level 0.415 Epoch 0: Train-Accuracy 0.926\n",
      "Epoch 200: Sat Level 0.518 Epoch 200: Train-Accuracy 0.983\n",
      "Epoch 400: Sat Level 0.562 Epoch 400: Train-Accuracy 0.990\n",
      "Epoch 600: Sat Level 0.562 Epoch 600: Train-Accuracy 0.990\n",
      "Epoch 800: Sat Level 0.562 Epoch 800: Train-Accuracy 0.990\n",
      "Training finished at Epoch 999 with Sat Level 0.562\n",
      "Epoch 0: Sat Level 0.516 Epoch 0: Train-Accuracy 0.976\n",
      "Epoch 200: Sat Level 0.541 Epoch 200: Train-Accuracy 0.988\n",
      "Epoch 400: Sat Level 0.541 Epoch 400: Train-Accuracy 0.988\n",
      "Epoch 600: Sat Level 0.541 Epoch 600: Train-Accuracy 0.988\n",
      "Epoch 800: Sat Level 0.541 Epoch 800: Train-Accuracy 0.988\n",
      "Training finished at Epoch 999 with Sat Level 0.541\n",
      "Epoch 0: Sat Level 0.529 Epoch 0: Train-Accuracy 0.983\n",
      "Epoch 200: Sat Level 0.548 Epoch 200: Train-Accuracy 0.989\n",
      "Epoch 400: Sat Level 0.548 Epoch 400: Train-Accuracy 0.989\n",
      "Epoch 600: Sat Level 0.548 Epoch 600: Train-Accuracy 0.989\n",
      "Epoch 800: Sat Level 0.548 Epoch 800: Train-Accuracy 0.989\n",
      "Training finished at Epoch 999 with Sat Level 0.548\n",
      "Epoch 0: Sat Level 0.530 Epoch 0: Train-Accuracy 0.983\n",
      "Epoch 200: Sat Level 0.548 Epoch 200: Train-Accuracy 0.989\n",
      "Epoch 400: Sat Level 0.548 Epoch 400: Train-Accuracy 0.989\n",
      "Epoch 600: Sat Level 0.570 Epoch 600: Train-Accuracy 0.991\n",
      "Epoch 800: Sat Level 0.570 Epoch 800: Train-Accuracy 0.991\n",
      "Training finished at Epoch 999 with Sat Level 0.570\n"
     ]
    }
   ],
   "source": [
    "D = ltn.Predicate.MLP([58],hidden_layer_sizes=(50,25))\n",
    "trainable_variables = D.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=5)\n",
    "\n",
    "kv = {}\n",
    "for k, (train, test) in enumerate(kf.split(Xs, ys)):\n",
    "    kv[\"posits{0}\".format(k)] = Xs[train][ys[train]==1].astype(np.float32)\n",
    "    kv[\"negats{0}\".format(k)] = Xs[train][ys[train]==2].astype(np.float32)\n",
    "    kv[\"Xtrain{0}\".format(k)] = Xs[train].astype(np.float32)\n",
    "    kv[\"ytrain{0}\".format(k)] = ys[train].astype(np.float32)\n",
    "    kv[\"Xtest{0}\".format(k)] = Xs[test].astype(np.float32)\n",
    "    kv[\"ytest{0}\".format(k)] = ys[test].astype(np.float32)\n",
    "    \n",
    "    var_posit = ltn.variable(\"posits\",kv[\"posits{0}\".format(k)])\n",
    "    var_negat = ltn.variable(\"negats\",kv[\"negats{0}\".format(k)])\n",
    "    oracle = Predpred(D)\n",
    "    \n",
    "    @tf.function\n",
    "    @ltn.domains()\n",
    "    def axioms():\n",
    "        axioms = []\n",
    "        weights = []\n",
    "        # forall data_A: A(data_A)\n",
    "        axioms.append(Forall(ltn.bound(var_posit), D(var_posit)))\n",
    "        # forall data_B: B(data_B)\n",
    "        axioms.append(Forall(ltn.bound(var_negat), Not(D(var_negat))))\n",
    "        axioms = tf.stack([tf.squeeze(ax) for ax in axioms])\n",
    "        sat_level = formula_aggregator(axioms)\n",
    "        return sat_level, axioms\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 1. - axioms()[0]\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        if epoch%200 == 0:\n",
    "            print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()[0]),\n",
    "                  \"Epoch %d: Train-Accuracy %.3f\"%(epoch, \n",
    "                                                   accuracy_score(kv[\"ytrain{0}\".format(k)],\n",
    "                                                                  oracle.predict(kv[\"Xtrain{0}\".format(k)]))))\n",
    "    print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()[0]))\n",
    "    kv[\"test_acc{0}\".format(k)] = accuracy_score(kv[\"ytest{0}\".format(k)], oracle.predict(kv[\"Xtest{0}\".format(k)]))\n",
    "    \n",
    "    dataset_orig_test_pred = dataset_orig.subset(test).copy(deepcopy=True)\n",
    "    dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.subset(test).features))\n",
    "    classified_metric_debiasing_test = ClassificationMetric(dataset_orig.subset(test), \n",
    "                                                     dataset_orig_test_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "    \n",
    "    kv[\"Disparate_impact{0}\".format(k)] = classified_metric_debiasing_test.disparate_impact()\n",
    "    kv[\"Parity_Difference{0}\".format(k)] = classified_metric_debiasing_test.statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average 5-fold accuracy as in Padala and Gujar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"test_acc0\"]+kv[\"test_acc1\"]+kv[\"test_acc2\"]+kv[\"test_acc3\"]+kv[\"test_acc4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average 5-fold Disparity Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9039553892656415"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Disparate_impact0\"]+kv[\"Disparate_impact1\"]+kv[\"Disparate_impact2\"]+kv[\"Disparate_impact3\"]+kv[\"Disparate_impact4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average 5-fold Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07063190582820023"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Parity_Difference0\"]+kv[\"Parity_Difference1\"]+kv[\"Parity_Difference2\"]+kv[\"Parity_Difference3\"]+kv[\"Parity_Difference4\"])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model - with bias - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Classification accuracy = 0.970000\n",
      "Dataset: Disparate impact = 0.897361\n",
      "Dataset: Parity Difference = -0.075269\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_test_pred = dataset_orig.copy(deepcopy=True)\n",
    "dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.features))\n",
    "display(Markdown(\"#### Model - with bias - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig, \n",
    "                                                 dataset_orig_test_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Dataset: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "print(\"Dataset: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Dataset: Parity Difference = %f\" % classified_metric_debiasing_test.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "C:\\Users\\Ben\\anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "X_df = pd.DataFrame(Xs,columns=dataset_orig.feature_names)\n",
    "X_r_preds = f(np.asarray(Xs).astype(np.float32))\n",
    "X_df['customer_risk_pred'] = X_r_preds\n",
    "X_female_df = X_df[X_df['sex'] == X_df['sex'].unique()[0]]\n",
    "X_male_df = X_df[X_df['sex'] == X_df['sex'].unique()[1]]\n",
    "X_female_df['customer class'] = pd.qcut(X_female_df['customer_risk_pred'],4, labels=[0,1,2],duplicates='drop')\n",
    "X_male_df['customer class'] = pd.qcut(X_male_df['customer_risk_pred'],4, labels=[0,1,2],duplicates='drop')\n",
    "X_inp = pd.concat([X_male_df,X_female_df])\n",
    "class1F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 0]\n",
    "class2F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 1]\n",
    "class3F = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 2]\n",
    "\n",
    "class1M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 0]\n",
    "class2M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[1]][X_inp['customer class'] == 1]\n",
    "class3M = X_inp[X_inp['sex'] == X_inp['sex'].unique()[0]][X_inp['customer class'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpclass1f = class1F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass2f = class2F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass3f = class3F.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "\n",
    "inpclass1m = class1M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass2m = class2M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "inpclass3m = class3M.iloc[:,:-2].astype(np.float32).to_numpy()\n",
    "\n",
    "var_class1f = ltn.variable(\"?class1F\",inpclass1f)\n",
    "var_class2f = ltn.variable(\"?class2F\",inpclass2f)\n",
    "var_class3f = ltn.variable(\"?class3F\",inpclass3f)\n",
    "\n",
    "var_class1m = ltn.variable(\"?class1M\",inpclass1m)\n",
    "var_class2m = ltn.variable(\"?class2M\",inpclass2m)\n",
    "var_class3m = ltn.variable(\"?class3M\",inpclass3m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = ltn.Predicate.MLP([58],hidden_layer_sizes=(50,25))\n",
    "trainable_variables = D.trainable_variables\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Sat Level 0.846 Epoch 0: Train-Accuracy 0.970\n",
      "Epoch 200: Sat Level 0.982 Epoch 200: Train-Accuracy 0.970\n",
      "Epoch 400: Sat Level 0.985 Epoch 400: Train-Accuracy 0.970\n",
      "Epoch 600: Sat Level 0.986 Epoch 600: Train-Accuracy 0.970\n",
      "Epoch 800: Sat Level 0.986 Epoch 800: Train-Accuracy 0.970\n",
      "Training finished at Epoch 999 with Sat Level 0.986 Epoch 999: Test-Accuracy 0.970\n",
      "Epoch 0: Sat Level 0.856 Epoch 0: Train-Accuracy 0.964\n",
      "Epoch 200: Sat Level 0.985 Epoch 200: Train-Accuracy 0.964\n",
      "Epoch 400: Sat Level 0.986 Epoch 400: Train-Accuracy 0.964\n",
      "Epoch 600: Sat Level 0.987 Epoch 600: Train-Accuracy 0.964\n",
      "Epoch 800: Sat Level 0.987 Epoch 800: Train-Accuracy 0.964\n",
      "Training finished at Epoch 999 with Sat Level 0.987 Epoch 999: Test-Accuracy 0.995\n",
      "Epoch 0: Sat Level 0.871 Epoch 0: Train-Accuracy 0.963\n",
      "Epoch 200: Sat Level 0.979 Epoch 200: Train-Accuracy 0.963\n",
      "Epoch 400: Sat Level 0.979 Epoch 400: Train-Accuracy 0.963\n",
      "Epoch 600: Sat Level 0.980 Epoch 600: Train-Accuracy 0.963\n",
      "Epoch 800: Sat Level 0.980 Epoch 800: Train-Accuracy 0.963\n",
      "Training finished at Epoch 999 with Sat Level 0.980 Epoch 999: Test-Accuracy 1.000\n",
      "Epoch 0: Sat Level 0.888 Epoch 0: Train-Accuracy 0.963\n",
      "Epoch 200: Sat Level 0.984 Epoch 200: Train-Accuracy 0.963\n",
      "Epoch 400: Sat Level 0.985 Epoch 400: Train-Accuracy 0.963\n",
      "Epoch 600: Sat Level 0.985 Epoch 600: Train-Accuracy 0.963\n",
      "Epoch 800: Sat Level 0.985 Epoch 800: Train-Accuracy 0.963\n",
      "Training finished at Epoch 999 with Sat Level 0.985 Epoch 999: Test-Accuracy 1.000\n",
      "Epoch 0: Sat Level 0.886 Epoch 0: Train-Accuracy 0.991\n",
      "Epoch 200: Sat Level 0.991 Epoch 200: Train-Accuracy 0.991\n",
      "Epoch 400: Sat Level 0.992 Epoch 400: Train-Accuracy 0.991\n",
      "Epoch 600: Sat Level 0.993 Epoch 600: Train-Accuracy 0.991\n",
      "Epoch 800: Sat Level 0.993 Epoch 800: Train-Accuracy 0.991\n",
      "Training finished at Epoch 999 with Sat Level 0.994 Epoch 999: Test-Accuracy 0.885\n"
     ]
    }
   ],
   "source": [
    "kv = {}\n",
    "for k, (train, test) in enumerate(kf.split(Xs, ys)):\n",
    "    kv[\"posits{0}\".format(k)] = Xs[train][ys[train]==1].astype(np.float32)\n",
    "    kv[\"negats{0}\".format(k)] = Xs[train][ys[train]==2].astype(np.float32)\n",
    "    kv[\"Xtrain{0}\".format(k)] = Xs[train].astype(np.float32)\n",
    "    kv[\"ytrain{0}\".format(k)] = ys[train].astype(np.float32)\n",
    "    kv[\"Xtest{0}\".format(k)] = Xs[test].astype(np.float32)\n",
    "    kv[\"ytest{0}\".format(k)] = ys[test].astype(np.float32)\n",
    "    \n",
    "\n",
    "    var_posit = ltn.variable(\"posits\",kv[\"posits{0}\".format(k)])\n",
    "    var_negat = ltn.variable(\"negats\",kv[\"negats{0}\".format(k)])\n",
    "    formula_aggregator = ltn.fuzzy_ops.Aggreg_pMeanError(p=2)\n",
    "\n",
    "    @tf.function\n",
    "    @ltn.domains()\n",
    "    def axioms():\n",
    "        axioms = []\n",
    "        weights = []\n",
    "        # forall data_A: A(data_A)\n",
    "        axioms.append(Forall(ltn.bound(var_posit), D(var_posit)))\n",
    "        weights.append(2.)\n",
    "        # forall data_B: B(data_B)\n",
    "        axioms.append(Forall(ltn.bound(var_negat), Not(D(var_negat))))\n",
    "        weights.append(2.)\n",
    "        # Equality Constraints\n",
    "        axioms.append(Forall(ltn.bound(var_class1f,var_class1m), Equiv(D(var_class1f),D(var_class1m)),p=2))\n",
    "        weights.append(1.5)\n",
    "        axioms.append(Forall(ltn.bound(var_class2f,var_class2m), Equiv(D(var_class2f),D(var_class2m)),p=2))\n",
    "        weights.append(1.5)\n",
    "        axioms.append(Forall(ltn.bound(var_class3f,var_class3m), Equiv(D(var_class3f),D(var_class3m)),p=2))\n",
    "        weights.append(1.5)\n",
    "        axioms = tf.stack([tf.squeeze(ax) for ax in axioms])\n",
    "        weights = tf.stack(weights)\n",
    "        weighted_axioms = weights*axioms\n",
    "        sat_level = formula_aggregator(weighted_axioms)\n",
    "        return sat_level, axioms\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 1. - axioms()[0]\n",
    "        grads = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        if epoch%200 == 0:\n",
    "            print(\"Epoch %d: Sat Level %.3f\"%(epoch, axioms()[0]),\n",
    "                  \"Epoch %d: Train-Accuracy %.3f\"%(epoch, \n",
    "                                                   accuracy_score(kv[\"ytrain{0}\".format(k)],\n",
    "                                                                  oracle.predict(kv[\"Xtrain{0}\".format(k)]))))\n",
    "    print(\"Training finished at Epoch %d with Sat Level %.3f\"%(epoch, axioms()[0]),\"Epoch %d: Test-Accuracy %.3f\"%(epoch, accuracy_score(kv[\"ytest{0}\".format(k)], oracle.predict(kv[\"Xtest{0}\".format(k)]))))\n",
    "    kv[\"test_acc{0}\".format(k)] = accuracy_score(kv[\"ytest{0}\".format(k)], oracle.predict(kv[\"Xtest{0}\".format(k)]))\n",
    "    \n",
    "    dataset_orig_test_pred = dataset_orig.subset(test).copy(deepcopy=True)\n",
    "    dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.subset(test).features))\n",
    "    classified_metric_debiasing_test = ClassificationMetric(dataset_orig.subset(test), \n",
    "                                                     dataset_orig_test_pred,\n",
    "                                                     unprivileged_groups=unprivileged_groups,\n",
    "                                                     privileged_groups=privileged_groups)\n",
    "    \n",
    "    kv[\"Disparate_impact{0}\".format(k)] = classified_metric_debiasing_test.disparate_impact()\n",
    "    kv[\"Parity_Difference{0}\".format(k)] = classified_metric_debiasing_test.statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"test_acc0\"]+kv[\"test_acc1\"]+kv[\"test_acc2\"]+kv[\"test_acc3\"]+kv[\"test_acc4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9024580904385917"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Disparate_impact0\"]+kv[\"Disparate_impact1\"]+kv[\"Disparate_impact2\"]+kv[\"Disparate_impact3\"]+kv[\"Disparate_impact4\"])/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Disparate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07230820743333258"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(kv[\"Parity_Difference0\"]+kv[\"Parity_Difference1\"]+kv[\"Parity_Difference2\"]+kv[\"Parity_Difference3\"]+kv[\"Parity_Difference4\"])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model - without bias - classification metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Classification accuracy = 0.970000\n",
      "Dataset: Disparate impact = 0.897361\n",
      "Dataset: Parity Difference = -0.075269\n"
     ]
    }
   ],
   "source": [
    "dataset_orig_test_pred = dataset_orig.copy(deepcopy=True)\n",
    "dataset_orig_test_pred.labels = oracle.predict(scale_orig.transform(dataset_orig.features))\n",
    "display(Markdown(\"#### Model - without bias - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(dataset_orig, \n",
    "                                                 dataset_orig_test_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Dataset: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "print(\"Dataset: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Dataset: Parity Difference = %f\" % classified_metric_debiasing_test.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPvjTe7yG+k8kXL2c8+ARTd",
   "collapsed_sections": [],
   "name": "LTN2_FairnessMeasures",
   "provenance": [
    {
     "file_id": "1iixzamRwIEOAxXzhtFq98EwKZaauIEkj",
     "timestamp": 1595253671546
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
